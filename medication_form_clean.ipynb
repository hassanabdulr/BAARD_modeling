{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28cd3b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os   \n",
    "import numpy as np\n",
    "import re, unicodedata\n",
    "import unicodedata\n",
    "from rapidfuzz import process, fuzz\n",
    "from typing import List\n",
    "from umlsparser import UMLSParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ea188",
   "metadata": {},
   "source": [
    "##preprocess and set up medication column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fcbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPT_neuro_medications = pd.read_excel('temp/raw/OPT N Meds Arm 6-8.xlsx', engine='openpyxl')[['record_id', \n",
    "        'date_med_entered', 'med_name',\n",
    "       'type_in_med', 'med_freq',\n",
    "       'med_freq_oth', 'med_start', 'non_psych_med_baseline___1', \n",
    "       'med_reason']]\n",
    "\n",
    "OPT_parent_medication = pd.read_csv('temp/raw/OPTIMUM_Medication_1.24.25.csv', skiprows=1)[['record_id', \n",
    "        'date_med_entered', 'med_name',\n",
    "       'type_in_med', 'med_freq',\n",
    "       'med_freq_oth', 'med_start', 'non_psych_med_baseline___1', \n",
    "       'med_reason']]\n",
    "\n",
    "# concat both df as OPT_medications\n",
    "\n",
    "OPT_medications = pd.concat([OPT_neuro_medications, OPT_parent_medication], \n",
    "                          axis=0,  # Concatenate vertically\n",
    "                          ignore_index=True)  \n",
    "\n",
    "# make a new column \"medication\" that takes either type_in_med or med_name if either is empty\n",
    "\n",
    "OPT_medications['medication'] = OPT_medications['type_in_med'].combine_first(OPT_medications['med_name']).str.upper()\n",
    "\n",
    "# drop rows where medication is empty\n",
    "OPT_medications = OPT_medications[OPT_medications['medication'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10826287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "# ------- regex vocab -------\n",
    "UNITS = r\"(mcg|Î¼g|ug|mg|g|gram|ml|mL|l|iu|units?|meq|mmol|%)\"\n",
    "FORM_WORDS = r\"(tab(let)?s?|cap(s|sules)?|susp(ension)?|sol(ution)?|syrup|drops?|spray|neb|cream|ointment|gel|patch|lozenge|supp(ository)?|inhaler|powder|granules?)\"\n",
    "ROUTE_WORDS = r\"(po|oral|iv|im|sc|subcut(an(eous)?)?|sl|subling(ual)?|top(ical)?|ophth(al)?|otic|nasal|intranasal|inhal(ed|ation)?|rectal|vaginal|transderm(al)?|buccal)\"\n",
    "FREQ_WORDS  = r\"(qd|od|qam|qpm|qhs|bid|tid|qid|q\\d+h|q\\d+hr|qod|prn|stat|hs|am|pm)\"\n",
    "REL_WORDS   = r\"(xr|sr|dr|er|cr|la|xl|ir)\"\n",
    "SALT_WORDS  = r\"(hcl|hydrochloride|hydrobromide|succinate|tartrate|maleate|mesylate|besylate|acetate|phosphate|sulfate|lactate|nitrate|carbonate|sodium|potassium|calcium|magnesium)\"\n",
    "\n",
    "DROP_TOKENS = {\n",
    "    \"TAB\",\"TABS\",\"TABLET\",\"TABLETS\",\"CAP\",\"CAPS\",\"CAPSULE\",\"CAPSULES\",\n",
    "    \"SOLUTION\",\"SUSP\",\"SUSPENSION\",\"SYRUP\",\"DROPS\",\"INJECTION\",\"TOPICAL\",\n",
    "    \"PATCH\",\"CREAM\",\"OINTMENT\",\"GEL\",\n",
    "    \"PRN\",\"BID\",\"TID\",\"QID\",\"QD\",\"OD\",\"QHS\",\"QAM\",\"QPM\",\"HS\",\"AM\",\"PM\",\n",
    "    \"PO\",\"IV\",\"IM\",\"SC\",\"SL\",\"SUBLINGUAL\",\"INHALER\",\"NEB\",\"NASAL\",\n",
    "    \"XR\",\"SR\",\"ER\",\"CR\",\"LA\",\"XL\",\"IR\",\n",
    "}\n",
    "\n",
    "LEADING_MODIFIERS = {\n",
    "    \"EXTENDED\", \"RELEASE\", \"EXTENDED-RELEASE\", \"SUSTAINED\", \"CONTROLLED\",\n",
    "    \"DELAYED\", \"IMMEDIATE\", \"PROLONGED\", \"ENTERIC\", \"COATED\", \"CHEWABLE\",\n",
    "    \"LONG\", \"ACTING\", \"ORAL\"  # sometimes appears as fluff\n",
    "}\n",
    "\n",
    "# patterns to *preserve whole token* if they match\n",
    "PROTECT_REGEXES = [\n",
    "    re.compile(r\"^VITAMIN\\s+[A-Z0-9]+(\\s+[A-Z0-9]+)?$\"),        # VITAMIN D, VITAMIN D3, VITAMIN B 12, etc.\n",
    "    re.compile(r\"^FISH\\s+OIL$\"),\n",
    "    re.compile(r\"^[A-Z]+\\s+OIL$\"),                               # e.g., CASTOR OIL (keeps both words)\n",
    "    re.compile(r\"^POLYETHYLENE\\s+GLYCOL(\\s+\\d+)?$\"),            # PEG / PEG 3350\n",
    "    re.compile(r\"^[A-Z]+\\s+(GLYCOL|ACID|PEROXIDE|ALCOHOL)$\"),   # generic 2-word chemicals\n",
    "]\n",
    "\n",
    "def _nfkc_upper(s: str) -> str:\n",
    "    s = \"\" if not isinstance(s, str) else s\n",
    "    return unicodedata.normalize(\"NFKC\", s).upper().strip()\n",
    "\n",
    "def _drop_parentheticals(s: str) -> str:\n",
    "    return re.sub(r\"\\([^)]*\\)\", \" \", s)\n",
    "\n",
    "def _drop_numbers_units(s: str) -> str:\n",
    "    s = re.sub(rf\"\\b\\d+(\\.\\d+)?\\s*(/{0,1}\\s*\\d+(\\.\\d+)?\\s*)*{UNITS}\\b\", \" \", s)\n",
    "    s = re.sub(rf\"\\b\\d+(\\.\\d+)?\\s*/\\s*\\d+(\\.\\d+)?\\b\", \" \", s)\n",
    "    s = re.sub(r\"\\b\\d+(\\.\\d+)?\\b\", \" \", s)  # stray numbers\n",
    "    return s\n",
    "\n",
    "def _drop_keywords(s: str) -> str:\n",
    "    for pat in [FORM_WORDS, ROUTE_WORDS, FREQ_WORDS, REL_WORDS, SALT_WORDS]:\n",
    "        s = re.sub(rf\"\\b{pat}\\b\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _normalize_separators(s: str) -> str:\n",
    "    s = s.replace(\"+\", \" + \").replace(\"/\", \" / \").replace(\"&\", \" & \")\n",
    "    s = re.sub(r\"[^\\w\\s\\+\\/&-]\", \" \", s)  # allow hyphen in things like EXTENDED-RELEASE\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def _tokenize_med(raw: str) -> List[str]:\n",
    "    s = _nfkc_upper(raw)\n",
    "    if not s: return []\n",
    "    s = _drop_parentheticals(s)\n",
    "    s = _drop_numbers_units(s)\n",
    "    s = _drop_keywords(s)\n",
    "    s = _normalize_separators(s)\n",
    "    if not s: return []\n",
    "    return re.split(r\"\\s*(?:/|&|\\+)\\s*\", s)\n",
    "\n",
    "def _strip_leading_modifiers(words: List[str]) -> List[str]:\n",
    "    # remove only leading fluff tokens\n",
    "    i = 0\n",
    "    while i < len(words) and (words[i] in LEADING_MODIFIERS or words[i] in DROP_TOKENS):\n",
    "        i += 1\n",
    "    return words[i:]\n",
    "\n",
    "def _is_protected_phrase(tok: str) -> bool:\n",
    "    return any(rx.match(tok) for rx in PROTECT_REGEXES)\n",
    "\n",
    "def clean_medication_string(raw: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of cleaned ingredient-like tokens.\n",
    "    - Removes dose/form/route/release/salts\n",
    "    - Splits combos on /, +, &\n",
    "    - Strips *leading* modifiers but preserves legitimate multiword ingredients\n",
    "    - De-dupes while preserving order\n",
    "    \"\"\"\n",
    "    parts = [p.strip() for p in _tokenize_med(raw) if p.strip()]\n",
    "    out: List[str] = []\n",
    "    for tok in parts:\n",
    "        # remove generic noise tokens, but only at the start; keep rest together\n",
    "        words = [w for w in tok.split() if w]  # keep all words for now\n",
    "        words = _strip_leading_modifiers(words)\n",
    "\n",
    "        # remove any remaining standalone DROP_TOKENS in the middle if they slipped through\n",
    "        words = [w for w in words if w not in DROP_TOKENS]\n",
    "\n",
    "        if not words:\n",
    "            continue\n",
    "\n",
    "        candidate = \" \".join(words)\n",
    "\n",
    "        # If it matches a protected multiword pattern (e.g., VITAMIN D3, FISH OIL), keep as-is\n",
    "        if _is_protected_phrase(candidate):\n",
    "            final = candidate\n",
    "        else:\n",
    "            # General rule: keep the full remaining phrase (NOT just the last word)\n",
    "            # This avoids truncating 'FISH OIL' -> 'OIL' or 'POLYETHYLENE GLYCOL' -> 'GLYCOL'\n",
    "            final = candidate\n",
    "\n",
    "        if final:\n",
    "            out.append(final)\n",
    "\n",
    "    # de-duplicate while preserving order\n",
    "    seen, dedup = set(), []\n",
    "    for x in out:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            dedup.append(x)\n",
    "    return dedup\n",
    "# ---- apply to your df (same as before) ----\n",
    "OPT_medications[\"clean_ingredients\"] = OPT_medications[\"medication\"].map(clean_medication_string)\n",
    "OPT_medications[\"clean_medication\"]  = OPT_medications[\"clean_ingredients\"].apply(\n",
    "    lambda xs: \" + \".join(xs) if xs else pd.NA\n",
    ")\n",
    "\n",
    "# optional: drop rows that failed to yield a cleaned token\n",
    "# OPT_medications = OPT_medications[OPT_medications[\"clean_medication\"].notna()]\n",
    "\n",
    "# optional: inspect coverage\n",
    "# print(OPT_medications[\"clean_medication\"].notna().mean())\n",
    "# print(OPT_medications[\"clean_medication\"].nunique(dropna=True))\n",
    "\n",
    "# optional: save distinct cleaned strings for a quick audit\n",
    "# (OPT_medications[\"clean_medication\"]\n",
    "#    .dropna()\n",
    "#    .drop_duplicates()\n",
    "#    .to_csv(\"OPT_cleaned_medication_unique.csv\", index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ee397",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPT_medications[\"clean_ingredients\"] = OPT_medications[\"medication\"].map(clean_medication_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f635af",
   "metadata": {},
   "source": [
    "### Open UMLS parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dac54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umlsparser import UMLSParser\n",
    "\n",
    "umls = UMLSParser('/external/rprshnas01/netdata_kcni/dflab/data/BAARD/code/resources/umls/2025AA')\n",
    "\n",
    "for cui, concept in umls.get_concepts().items():\n",
    "    if 'ICD10CM' in concept.get_source_ids().keys():\n",
    "        icd10ids = concept.get_source_ids().get('ICD10CM')\n",
    "        print(icd10ids, concept.get_preferred_names_for_language('ENG')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umlsparser import UMLSParser\n",
    "import collections\n",
    "\n",
    "umls = UMLSParser('resources/umls/2025AA')\n",
    "sources_counter = collections.defaultdict(int)\n",
    "for cui, concept in umls.get_concepts().items():\n",
    "    sources = concept.get_source_ids().keys()\n",
    "    for source in sources:\n",
    "        sources_counter[source] += 1\n",
    "print('|SOURCE|COUNT|\\n|------|-----|')\n",
    "for source, count in sorted(sources_counter.items(), key=lambda t: t[1], reverse=True):\n",
    "    print('|{}|{}|'.format(source, count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7df0ecc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:umlsparser.UMLSParser:Initialising UMLSParser for basepath C:\\Users\\Hassan\\Documents\\Projects\\baard\\resources\\umls\\2025AA\n",
      "INFO:umlsparser.UMLSParser:No language filtering applied.\n",
      "Parsing UMLS concepts (MRCONSO.RRF): 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 4668: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m umls = \u001b[43mUMLSParser\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mHassan\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mDocuments\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mProjects\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mbaard\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mresources\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mumls\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43m2025AA\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cui, concept \u001b[38;5;129;01min\u001b[39;00m umls.get_concepts().items():\n\u001b[32m      4\u001b[39m     tui = concept.get_tui()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hassan\\Documents\\Projects\\baard\\umlsparser\\UMLSParser.py:54\u001b[39m, in \u001b[36mUMLSParser.__init__\u001b[39m\u001b[34m(self, path, language_filter)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mself\u001b[39m.concepts = {}\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m.semantic_types = {}\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__parse_mrconso__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mself\u001b[39m.__parse_mrdef__()\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m.__parse_mrsty__()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hassan\\Documents\\Projects\\baard\\umlsparser\\UMLSParser.py:70\u001b[39m, in \u001b[36mUMLSParser.__parse_mrconso__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__parse_mrconso__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMRCONSO\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mParsing UMLS concepts (MRCONSO.RRF)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m|\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCUI\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLAT\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# language of term\u001b[39;49;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCVF\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m17\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hassan\\Documents\\Projects\\baard\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\encodings\\cp1252.py:23\u001b[39m, in \u001b[36mIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs.charmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m.errors,decoding_table)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'charmap' codec can't decode byte 0x81 in position 4668: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "umls = UMLSParser('C:\\\\Users\\\\Hassan\\\\Documents\\\\Projects\\\\baard\\\\resources\\\\umls\\\\2025AA')\n",
    "\n",
    "for cui, concept in umls.get_concepts().items():\n",
    "    tui = concept.get_tui()\n",
    "    name_of_semantic_type = umls.get_semantic_types()[concept.get_tui()].get_name()\n",
    "    for name in concept.get_names_for_language('ENG'):\n",
    "        print(cui, name, tui, name_of_semantic_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08269212",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59788f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
